{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the JSON file and extracting the relevant Tweets/info, Then inserting into the MongoDB collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessry libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo #MongoDB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB Connection\n",
    "# Connect to MongoDB database\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"Twitter_Database1\"]\n",
    "tweet_collection = mongo_db[\"Tweets1\"]\n",
    "quoted_tweet_collection = mongo_db[\"Quoted-Tweets1\"]\n",
    "retweet_collection = mongo_db[\"Retweets1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tweet(tweet_info,collec):\n",
    "    \"\"\"Insert a tweet document into the MongoDB collection.\"\"\"\n",
    "    try:\n",
    "        collec.insert_one(tweet_info)\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting tweet: {e}\")\n",
    "\n",
    "def find_tweet1(tweet_id,collec):\n",
    "    \"\"\"Check if a tweet with given ID exists in the MongoDB collection.\"\"\"\n",
    "    tweet = collec.find_one({\"Tweet_Id\": tweet_id})\n",
    "    return tweet is not None\n",
    "\n",
    "def find_tweet(tweet_id):\n",
    "    \"\"\"\n",
    "    Check if a tweet with the given ID exists in any of the MongoDB collections.\n",
    "    \"\"\"\n",
    "    for collection in [tweet_collection,retweet_collection,quoted_tweet_collection]:\n",
    "        tweet = collection.find_one({\"Tweet_Id\": tweet_id})\n",
    "        if tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def update_tweet(data, collection):\n",
    "    \"\"\"\n",
    "    Update a tweet document in the MongoDB collection if new counts are higher than existing counts.\n",
    "    \"\"\"\n",
    "    tweet_info = extract_tweet_info(data)\n",
    "    existing_tweet = collection.find_one({\"Tweet_Id\": tweet_info['Tweet_Id']})\n",
    "    if existing_tweet:\n",
    "        # Compare and update counts if new counts are higher\n",
    "        for field in ['Retweet_Count', 'Quote_count', 'Likes_Count']:\n",
    "            if tweet_info[field] > existing_tweet.get(field, 0):\n",
    "                existing_tweet[field] = tweet_info[field]\n",
    "        # Update the document in the collection\n",
    "        collection.update_one({\"Tweet_Id\": tweet_info['Tweet_Id']}, {\"$set\": existing_tweet})\n",
    "        return  # Exit loop after updating the first found document\n",
    "\n",
    "def delete_tweet(tweet_id, collec):\n",
    "    \"\"\"Delete a tweet document from the MongoDB collection.\"\"\"\n",
    "    try:\n",
    "        result = collec.delete_one({\"Tweet_Id\": tweet_id})\n",
    "        if result.deleted_count == 1:\n",
    "            print(f\"Tweet with ID {tweet_id} deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"No tweet found with ID {tweet_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting tweet: {e}\")\n",
    "\n",
    "def extract_tweet_info(data):\n",
    "    \"\"\"Extract relevant information from tweet data.\"\"\"\n",
    "    try:\n",
    "        created_at = datetime.strptime(data['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    except KeyError:\n",
    "        created_at = None\n",
    "    \n",
    "    try:\n",
    "        hashtag_list = [hashtag['text'] for hashtag in data['entities']['hashtags']]\n",
    "    except KeyError:\n",
    "        hashtag_list = []\n",
    "\n",
    "    return {\n",
    "        'created_at':datetime.strftime(datetime.strptime(data['created_at'],'%a %b %d %H:%M:%S +0000 %Y'),'%Y-%m-%d %H:%M:%S'),\n",
    "        'Tweet_Id':data['id_str'],\n",
    "        'Text':data['text'],\n",
    "        'Hashtag':list(map(lambda x: x[\"text\"], data['entities']['hashtags'])),\n",
    "        'User_Id':data['user']['id_str'],\n",
    "        'User_Name':data['user']['name'],\n",
    "        'User_Screen_Name': data['user']['screen_name'],\n",
    "        'Retweet_Count': data['retweet_count'],\n",
    "        'Quote_count': data['quote_count'],\n",
    "        'Likes_Count': data['favorite_count']\n",
    "    }\n",
    "\n",
    "def find_duplicate_tweets(tweet_id, collec):\n",
    "    \"\"\"Find duplicate tweets with the same Tweet_Id.\"\"\"\n",
    "    duplicate_tweets = collec.find({\"Tweet_Id\": tweet_id})\n",
    "    return list(duplicate_tweets)\n",
    "\n",
    "def find_duplicate_tweets(collec):\n",
    "    \"\"\"Find duplicate tweets based on content or Tweet_Id.\"\"\"\n",
    "    pipeline = [\n",
    "        {\"$group\": {\"_id\": \"$Tweet_Id\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "    ]\n",
    "    duplicate_tweets = collec.aggregate(pipeline)\n",
    "    return list(duplicate_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(file_path):\n",
    "    \"\"\"Process tweets from a JSON file and insert them into the MongoDB collection.\"\"\"\n",
    "    with open(file_path, \"r\") as read_file:\n",
    "        for line_number,line in enumerate(read_file,start=1):\n",
    "            if line_number%2 !=0:\n",
    "                data = json.loads(line)\n",
    "                # check if tweet already exists\n",
    "                if find_tweet1(data['id_str'],tweet_collection):\n",
    "                    update_tweet(data,tweet_collection)\n",
    "                    continue\n",
    "                if find_tweet1(data['id_str'],retweet_collection):\n",
    "                    update_tweet(data,retweet_collection)\n",
    "                    continue\n",
    "                if find_tweet1(data['id_str'],quoted_tweet_collection):\n",
    "                    update_tweet(data,quoted_tweet_collection)\n",
    "                    continue\n",
    "\n",
    "                #check if retweet\n",
    "                if ( data['text'].startswith('RT') ):\n",
    "                    d1 = extract_tweet_info(data)\n",
    "                    try:\n",
    "                        d1['source_id'] = data['retweeted_status']['id_str']\n",
    "                    except:\n",
    "                        d1['source_id'] = 'NotFound'\n",
    "                    insert_tweet(d1,retweet_collection)\n",
    "                    if 'quoted_status' in data:\n",
    "                        if find_tweet(data['quoted_status']['id_str']):\n",
    "                            update_tweet(data['quoted_status'],tweet_collection)\n",
    "                            continue\n",
    "                        d2= extract_tweet_info(data['quoted_status'])\n",
    "                        insert_tweet(d2,tweet_collection)\n",
    "                        if 'retweeted_status' in data:\n",
    "                            if find_tweet(data['retweeted_status']['id_str']):\n",
    "                                update_tweet(data['retweeted_status'],quoted_tweet_collection)\n",
    "                                continue\n",
    "                            d3 = extract_tweet_info(data['retweeted_status'])\n",
    "                            d3['source_id'] = data['quoted_status']['id_str']\n",
    "                            insert_tweet(d3,quoted_tweet_collection)\n",
    "                    if 'retweeted_status' in data and data['is_quote_status'] == False:\n",
    "                        if find_tweet(data['retweeted_status']['id_str']):\n",
    "                            update_tweet(data['retweeted_status'],tweet_collection)\n",
    "                            continue\n",
    "                        d2 = extract_tweet_info(data['retweeted_status'])\n",
    "                        insert_tweet(d2,tweet_collection)\n",
    "\n",
    "                #check if quoted tweet\n",
    "                elif 'quoted_status' in data and (data['text'].startswith('RT') is False):\n",
    "                    d3 = extract_tweet_info(data)\n",
    "                    d3['source_id'] = data['quoted_status']['id_str']\n",
    "                    insert_tweet(d3,quoted_tweet_collection)\n",
    "                    if find_tweet(data['quoted_status']['id_str']):\n",
    "                        update_tweet(data['quoted_status'],tweet_collection)\n",
    "                        continue\n",
    "                    d2 = extract_tweet_info(data['quoted_status'])\n",
    "                    insert_tweet(d2,tweet_collection)\n",
    "                    \n",
    "                #if only tweet\n",
    "                else:\n",
    "                    d2 = extract_tweet_info(data)\n",
    "                    insert_tweet(d2,tweet_collection)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tweets('corona-out-2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tweets('corona-out-3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the tweets further for Advanced Search Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing for Advanced Search\n",
    "import iso639\n",
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "from langdetect import detect_langs\n",
    "from translate import Translator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))  # English stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get language code from language name\n",
    "def get_language_code(lang_name):\n",
    "    try:\n",
    "        lang_code = iso639.languages.get(part3b=lang_name).part1\n",
    "        return lang_code\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Function to get synonyms for a word\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return synonyms\n",
    "\n",
    "# Function to translate individual words to English\n",
    "def translate_words_to_english(words, lang_code):\n",
    "    translator = Translator(to_lang=\"en\", from_lang=lang_code)\n",
    "    translated_words = [translator.translate(word) for word in words]\n",
    "    return translated_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and punctuation, preserving certain symbols\n",
    "    preserved_symbols = [\"@\", \"#\", \".\", \"_\"]  # Add any other symbols to be preserved\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in string.punctuation or any(symbol in token for symbol in preserved_symbols)]\n",
    "    # Remove numbers\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    # Lemmatization and synonym inclusion\n",
    "    lemmatized_tokens_with_synonyms = []\n",
    "    for token in tokens:\n",
    "        lemmatized_tokens_with_synonyms.append(token)\n",
    "        synonyms = get_synonyms(token)\n",
    "        lemmatized_tokens_with_synonyms.extend(list(synonyms))\n",
    "    return list(set(lemmatized_tokens_with_synonyms))\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    if sentiment_score[\"compound\"] >= 0.55:\n",
    "        result = \"Positive\"\n",
    "    elif sentiment_score[\"compound\"] <= 0.45:\n",
    "        result = \"Negative\"\n",
    "    else:\n",
    "        result = \"Neutral\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeText(collection):\n",
    "    # Process text and save to new field\n",
    "    for tweet in collection.find():\n",
    "        tweet_text = tweet[\"Text\"]\n",
    "        try:\n",
    "            detected_langs = detect_langs(tweet_text)  # Detect language using langdetect\n",
    "            lang_name = detected_langs[0].lang  # Get the most likely language name\n",
    "            print(f\"Detected language: {lang_name}\")  # Debug print for detected language\n",
    "            if lang_name == 'en':  # English tweets\n",
    "                processed_text = preprocess_text(tweet_text)\n",
    "                sentiment = sentiment_analysis(tweet_text)\n",
    "                collection.update_one({\"_id\": tweet[\"_id\"]}, {\"$set\": {\"processed_text\": processed_text, \"language\": lang_name, \"sentiment\": sentiment}})\n",
    "                print(f\"Processed text for tweet {tweet['_id']}\")\n",
    "            else:\n",
    "                if lang_name is not None:\n",
    "                    words = word_tokenize(tweet_text)\n",
    "                    translated_words = translate_words_to_english(words, lang_name)\n",
    "                    translated_text = ' '.join(translated_words)\n",
    "                    processed_text = preprocess_text(translated_text)\n",
    "                    sentiment = sentiment_analysis(tweet_text)\n",
    "                    collection.update_one({\"_id\": tweet[\"_id\"]}, {\"$set\": {\"processed_text\": processed_text, \"language\": lang_name, \"sentiment\": sentiment}})\n",
    "                    print(f\"Processed text for tweet {tweet['_id']}\")\n",
    "                else:\n",
    "                    print(f\"Unsupported language: {lang_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tweet {tweet['_id']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeText(tweet_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeText(retweet_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeText(quoted_tweet_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
